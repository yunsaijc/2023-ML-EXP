{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数学基础\n",
    "\n",
    "线性代数/ 微积分/ 自动微分/ 概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量\n",
    "\n",
    "标量, 向量, 矩阵: 略\n",
    "\n",
    "张量: 更加广泛意义上的向量. 即: 向量是1维张量, 矩阵是2维张量. 那么张量可以是n (n>2)维的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "\n",
      "A + 1:  tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.],\n",
      "        [17., 18., 19., 20.]])\n",
      "\n",
      "A * 2:  tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [16., 18., 20., 22.],\n",
      "        [24., 26., 28., 30.],\n",
      "        [32., 34., 36., 38.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()\n",
    "print(\"A: \", A, end=\"\\n\\n\")\n",
    "\n",
    "# 标量与张量的运算\n",
    "# 张量+标量: 每个元素都加上标量\n",
    "print(\"A + 1: \", A + 1, end=\"\\n\\n\")\n",
    "\n",
    "# 张量*标量: 每个元素都乘以标量\n",
    "print(\"A * 2: \", A * 2, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A * B:  tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.],\n",
      "        [144., 169., 196., 225.],\n",
      "        [256., 289., 324., 361.]])\n",
      "\n",
      "A @ B.T:  tensor([[  14.,   38.,   62.,   86.,  110.],\n",
      "        [  38.,  126.,  214.,  302.,  390.],\n",
      "        [  62.,  214.,  366.,  518.,  670.],\n",
      "        [  86.,  302.,  518.,  734.,  950.],\n",
      "        [ 110.,  390.,  670.,  950., 1230.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 张量与张量的运算\n",
    "# Hadamard 积: 对应位置的元素相乘\n",
    "print(\"A * B: \", A * B, end=\"\\n\\n\")\n",
    "\n",
    "# 矩阵乘法\n",
    "print(\"A @ B.T: \", A @ B.T, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维与非降维的求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接对张量进行求和/ 求均值, 会返回一个标量. 实际上是一种降维的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A:  torch.Size([5, 4])\n",
      "\n",
      "Sum of A:  tensor(190.)\n",
      "\n",
      "Mean of A:  tensor(9.5000)\n",
      "\n",
      "Mean of A:  tensor(9.5000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of A: \", A.shape, end=\"\\n\\n\")\n",
    "print(\"Sum of A: \", A.sum(), end=\"\\n\\n\")\n",
    "print(\"Mean of A: \", A.mean(), end=\"\\n\\n\")\n",
    "print(\"Mean of A: \", A.sum() / A.numel(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述的默认求和/ 求均值是沿着所有轴来降低维度, 从而最终都汇聚到一个标量中\n",
    "\n",
    "其实还可以指定某个轴进行求和, 也就是指定某个轴去降低维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A:  torch.Size([5, 4])\n",
      "\n",
      "Sum by axis 0:  tensor([40., 45., 50., 55.])\n",
      "Shape of sum by axis 0:  torch.Size([4])\n",
      "\n",
      "Sum by axis 1:  tensor([ 6., 22., 38., 54., 70.])\n",
      "Shape of sum by axis 1:  torch.Size([5])\n",
      "\n",
      "Sum by axis 0 and 1:  tensor(190.)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of A: \", A.shape, end=\"\\n\\n\")\n",
    "\n",
    "# 沿第0维求和, shape中的第0维消失\n",
    "print(\"Sum by axis 0: \", A.sum(axis=0))\n",
    "print(\"Shape of sum by axis 0: \", A.sum(axis=0).shape, end=\"\\n\\n\")\n",
    "\n",
    "# 沿着第1维求和, shape中的第1维消失\n",
    "print(\"Sum by axis 1: \", A.sum(axis=1))\n",
    "print(\"Shape of sum by axis 1: \", A.sum(axis=1).shape, end=\"\\n\\n\")\n",
    "\n",
    "# 沿着第0维和第1维求和, shape中的第0维和第1维消失\n",
    "# 与A.sum()等价\n",
    "print(\"Sum by axis 0 and 1: \", A.sum(axis=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理, 均值也可以指定某个轴进行求值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean by axis 0:  tensor([ 8.,  9., 10., 11.])\n",
      "Mean by axis 0:  tensor([ 8.,  9., 10., 11.])\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean by axis 0: \", A.mean(axis=0))\n",
    "print(\"Mean by axis 0: \", A.sum(axis=0) / A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外, 还可以在保持轴数不变时进行非降维的求和. \n",
    "\n",
    "可以发现只有指定的维度shape变为1, 其他维度shape不变, 总的shape也不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A:  torch.Size([5, 4])\n",
      "\n",
      "Sum by axis 0 and keepdims: \n",
      " tensor([[40., 45., 50., 55.]])\n",
      "Shape of sum by axis 0 and keepdims: \n",
      " torch.Size([1, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of A: \", A.shape, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Sum by axis 0 and keepdims: \\n\", A.sum(axis=0, keepdims=True))\n",
    "print(\"Shape of sum by axis 0 and keepdims: \\n\", A.sum(axis=0, keepdims=True).shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以使用`cumsum()`函数, 保持所有维度的shape不变, 返回一个累加的张量\n",
    "\n",
    "`A[i] = A[i-1] + A[i-2] + ... + A[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumsum by axis 0: \n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"cumsum by axis 0: \\n\", A.cumsum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非降维求和有一些特定的用处:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "\n",
      "Sum by axis 1 and keepdims: \n",
      " tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "\n",
      "A / A.sum(axis=1, keepdims=True): \n",
      " tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
      "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
      "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
      "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
      "        [0.2286, 0.2429, 0.2571, 0.2714]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"A: \", A, end=\"\\n\\n\")\n",
    "print(\"Sum by axis 1 and keepdims: \\n\", A.sum(axis=1, keepdims=True), end=\"\\n\\n\")\n",
    "\n",
    "# 利用广播机制来计算每个元素的比例\n",
    "print(\"A / A.sum(axis=1, keepdims=True): \\n\", A / A.sum(axis=1, keepdims=True), end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of x and y: \n",
      " tensor(6.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 点积: 两个向量 (1维张量) 的积\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "y = torch.ones(4, dtype=torch.float32)\n",
    "print(\"Dot product of x and y: \\n\", torch.dot(x, y), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A:  torch.Size([5, 4])\n",
      "Shape of x:  torch.Size([4])\n",
      "\n",
      "A @ x: \n",
      " tensor([ 14.,  38.,  62.,  86., 110.])\n",
      "\n",
      "A @ x: \n",
      " tensor([ 14.,  38.,  62.,  86., 110.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 向量与矩阵的积\n",
    "print(\"Shape of A: \", A.shape)\n",
    "print(\"Shape of x: \", x.shape, end=\"\\n\\n\")\n",
    "\n",
    "print(\"A @ x: \\n\", torch.mv(A, x), end=\"\\n\\n\")\n",
    "print(\"A @ x: \\n\", A @ x, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A @ B^T: \n",
      " tensor([[  14.,   38.,   62.,   86.,  110.],\n",
      "        [  38.,  126.,  214.,  302.,  390.],\n",
      "        [  62.,  214.,  366.,  518.,  670.],\n",
      "        [  86.,  302.,  518.,  734.,  950.],\n",
      "        [ 110.,  390.,  670.,  950., 1230.]])\n",
      "\n",
      "A @ B^T: \n",
      " tensor([[  14.,   38.,   62.,   86.,  110.],\n",
      "        [  38.,  126.,  214.,  302.,  390.],\n",
      "        [  62.,  214.,  366.,  518.,  670.],\n",
      "        [  86.,  302.,  518.,  734.,  950.],\n",
      "        [ 110.,  390.,  670.,  950., 1230.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 矩阵与矩阵的积\n",
    "print(\"A @ B^T: \\n\", torch.mm(A, B.T), end=\"\\n\\n\")\n",
    "print(\"A @ B^T: \\n\", A @ B.T, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([0., 1., 2., 3.])\n",
      "\n",
      "L2 norm of x: \n",
      " tensor(3.7417)\n",
      "\n",
      "L1 norm of x: \n",
      " tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "print(\"x: \", x, end=\"\\n\\n\")\n",
    "\n",
    "# L2范数\n",
    "print(\"L2 norm of x: \\n\", torch.norm(x), end=\"\\n\\n\")\n",
    "\n",
    "# L1范数\n",
    "print(\"L1 norm of x: \\n\", torch.abs(x).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般地, $L_p$范数的形式为:\n",
    "$$||x||_p=(\\sum_{i=1}^{n}  |x_i|^p)^{1/p}$$\n",
    "\n",
    "矩阵$X$的Fronebius范数类似于向量的$L_2$范数: \n",
    "$$||X||_F=\\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}x^2_{ij}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "\n",
      "Frobenius norm of A: \n",
      " tensor(49.6991)\n"
     ]
    }
   ],
   "source": [
    "# 矩阵的Frobenius范数\n",
    "print(\"A: \", A, end='\\n\\n')\n",
    "print(\"Frobenius norm of A: \\n\", torch.norm(A))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
