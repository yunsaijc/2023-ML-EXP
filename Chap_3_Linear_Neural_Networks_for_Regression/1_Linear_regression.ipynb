{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归的基本元素\n",
    "\n",
    "理论内容见参考内容[1][2], 此略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矢量化加速\n",
    "\n",
    "训练模型时一般同时处理一个批次 (Batch) 的样本, 使用矢量化来加速计算. \n",
    "\n",
    "例如, 样本数为 $n$, 特征数为 $d$, 则训练数据的矩阵为 $X \\in \\mathbb{R}^{n \\times d}$, 标签的向量 $y \\in \\mathbb{R}^{n \\times 1}$, 参数的向量 $w \\in \\mathbb{R}^{d \\times 1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of <for> loop to add two vectors: 0.01205 s\n",
      "Time of vectorization to add two vectors: 0.00029 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "n = 1000\n",
    "a = torch.ones(n)\n",
    "b = torch.ones(n)\n",
    "\n",
    "# for循环 for loop\n",
    "c = torch.zeros(n)\n",
    "start = time.time()\n",
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]\n",
    "print(\"Time of <for> loop to add two vectors: {:.5f} s\".format(time.time()-start))\n",
    "\n",
    "# 矢量化 vectorization\n",
    "start = time.time()\n",
    "d = a + b\n",
    "print(\"Time of vectorization to add two vectors: {:.5f} s\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正态分布与平方损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下解释`噪声`与`平方损失目标函数`的关系\n",
    "\n",
    "正态分布概率密度函数: $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)$\n",
    "\n",
    "均方损失可以用于线性回归, 因为假设噪声服从正态分布, 即: $y = w^Tx + b+\\epsilon$, 其中$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "那么给定$x$观测到特定的$y$的似然就是: $P(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)$\n",
    "\n",
    "优化过程:\n",
    "\n",
    "找到最优的$w$和$b$来最大化似然函数, 也就是最小化负对数似然函数: $-\\log P(y|x) = \\frac{1}{2\\sigma^2}(y-w^Tx-b)^2 + \\log\\sqrt{2\\pi\\sigma^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归与深度网络\n",
    "\n",
    "线性回归可以看作一个单层神经网络\n",
    "\n",
    "这种每个输入与每个输出相连的网络叫全连接层"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
