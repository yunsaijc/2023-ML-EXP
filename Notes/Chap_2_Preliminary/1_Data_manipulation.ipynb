{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本的数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x:\n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\n",
      "Shape of x: torch.Size([12])\n",
      "\n",
      "Total number of elements in x: 12\n",
      "Reshaped Tensor X:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "Reshaped Tensor Y:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "Reshaped Tensor Z:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一维张量 (array of range)\n",
    "# 用法与python的range()函数相同\n",
    "x = torch.arange(12)\n",
    "print(\"Tensor x:\\n\", x, end=\"\\n\\n\")\n",
    "\n",
    "# 查看张量形状\n",
    "print(\"Shape of x:\", x.shape, end=\"\\n\\n\")\n",
    "# 查看张量元素总数\n",
    "print(\"Total number of elements in x:\", x.numel())  # num of elements\n",
    "\n",
    "# 改变张量形状\n",
    "X = x.reshape(3, 4)\n",
    "print(\"Reshaped Tensor X:\\n\", X, end=\"\\n\\n\")\n",
    "# 可以使用-1来自动推断张量形状\n",
    "Y = x.reshape(-1, 4)\n",
    "print(\"Reshaped Tensor Y:\\n\", Y, end=\"\\n\\n\")\n",
    "Z = x.reshape(3, -1)\n",
    "print(\"Reshaped Tensor Z:\\n\", Z, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是对一维张量的创建与操作. 以下对矩阵 (多维张量) 进行创建与操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All zeros:\n",
      " tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "\n",
      "All ones:\n",
      " tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "\n",
      "Random numbers:\n",
      " tensor([[ 1.7909,  1.0899, -1.8142,  0.7892],\n",
      "        [-0.6737, -0.2567,  0.5218, -0.3231],\n",
      "        [-1.4233, -0.1328, -0.3276, -0.2521]])\n",
      "\n",
      "Given data:\n",
      " tensor([[2, 1, 4, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [4, 3, 2, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建指定形状的张量\n",
    "print(\"All zeros:\\n\", torch.zeros((2, 3, 4)), end=\"\\n\\n\")\n",
    "print(\"All ones:\\n\", torch.ones((2, 3, 4)), end=\"\\n\\n\")\n",
    "print(\"Random numbers:\\n\", torch.randn(3, 4), end=\"\\n\\n\")\n",
    "print(\"Given data:\\n\", torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以通过逻辑运算的方式，来创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "Y:\n",
      " tensor([[0, 1, 2],\n",
      "        [5, 4, 3]])\n",
      "\n",
      "X == Y:\n",
      " tensor([[ True,  True,  True],\n",
      "        [False,  True, False]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(6).reshape(2, 3)\n",
    "Y = torch.tensor([[0, 1, 2], [5, 4, 3]])\n",
    "print(\"X:\\n\", X, end=\"\\n\\n\")\n",
    "print(\"Y:\\n\", Y, end=\"\\n\\n\")\n",
    "print(\"X == Y:\\n\", X == Y, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对两个相同形状的张量做运算, 会按元素做相应运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y = tensor([ 3,  4,  6, 10])\n",
      "\n",
      "x - y = tensor([-1,  0,  2,  6])\n",
      "\n",
      "x * y = tensor([ 2,  4,  8, 16])\n",
      "\n",
      "x / y = tensor([0.5000, 1.0000, 2.0000, 4.0000])\n",
      "\n",
      "x ** y = tensor([ 1,  4, 16, 64])\n",
      "\n",
      "e ^ x = tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n",
      "\n",
      "Sum of x = tensor(15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "print(\"x + y =\", x + y, end=\"\\n\\n\")\n",
    "print(\"x - y =\", x - y, end=\"\\n\\n\")\n",
    "print(\"x * y =\", x * y, end=\"\\n\\n\")\n",
    "print(\"x / y =\", x / y, end=\"\\n\\n\")\n",
    "print(\"x ** y =\", x ** y, end=\"\\n\\n\")\n",
    "print(\"e ^ x =\", torch.exp(x), end=\"\\n\\n\")\n",
    "print(\"Sum of x =\", torch.sum(x), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "\n",
      "Y:\n",
      " tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n",
      "\n",
      "Concatenate on axis 0:\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "\n",
      "Concatenate on axis 1:\n",
      " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 拼接张量 (concatenate)\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(\"X:\\n\", X, end=\"\\n\\n\")\n",
    "print(\"Y:\\n\", Y, end=\"\\n\\n\")\n",
    "print(\"Concatenate on axis 0:\\n\", torch.cat((X, Y), dim=0), end=\"\\n\\n\")\n",
    "print(\"Concatenate on axis 1:\\n\", torch.cat((X, Y), dim=1), end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of concatenated tensor on axis 0: torch.Size([6, 4])\n",
      "\n",
      "Shape of concatenated tensor on axis 1: torch.Size([3, 8])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加深对dim的理解\n",
    "# dim的数字从小到大，表示从外到内\n",
    "# dim=n, 那么结果的shape的第n维 = 被拼接向量的第n维相加\n",
    "\n",
    "# dim=0, 两者的shape第0维的数字相加, [3+3, 4]\n",
    "print(\"Shape of concatenated tensor on axis 0:\", torch.cat((X, Y), dim=0).shape, end=\"\\n\\n\")\n",
    "# dim=1, 两者的shape第1维的数字相加, [3, 4+4]\n",
    "print(\"Shape of concatenated tensor on axis 1:\", torch.cat((X, Y), dim=1).shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 3 but got size 4 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m16\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcat((X, Y), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# ok\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# error\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 3 but got size 4 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# 因此, 若要在dim=n上拼接, 那么两者除了dim=n的维度外, 其他维度的shape必须相同\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.arange(16, dtype=torch.float32).reshape((4, 4))\n",
    "torch.cat((X, Y), dim=0)  # ok\n",
    "torch.cat((X, Y), dim=1)  # error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 广播机制\n",
    "\n",
    "上述操作是在两个形状相同的张量中进行的. 但在形状不同的时候, 其实可以使用广播机制来按元素进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "\n",
      "b:\n",
      " tensor([[0, 1]])\n",
      "\n",
      "a + b:\n",
      " tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n",
      "\n",
      "Shape of a + b: torch.Size([3, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "print(\"a:\\n\", a, end=\"\\n\\n\")\n",
    "print(\"b:\\n\", b, end=\"\\n\\n\")\n",
    "\n",
    "# 可以看作b的每一列都分别加上a的每一行\n",
    "print(\"a + b:\\n\", a + b, end=\"\\n\\n\")\n",
    "\n",
    "# shape: [3, 1] + [1, 2] = [3, 2]\n",
    "print(\"Shape of a + b:\", (a + b).shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[-1] =  tensor([ 8,  9, 10, 11])\n",
      "\n",
      "X[1:3] =  tensor([[ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "X[1, 2] =  tensor(6)\n",
      "\n",
      "X[1:3, 2:4] = \n",
      " tensor([[ 6,  7],\n",
      "        [10, 11]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 与python list操作一样\n",
    "print(\"X[-1] = \", X[-1], end=\"\\n\\n\")    # 最后一行, 即第1维的最后一个元素\n",
    "print(\"X[1:3] = \", X[1:3], end=\"\\n\\n\")  # 第1, 2行, 即第1维的第1个元素到第3个元素\n",
    "print(\"X[1, 2] = \", X[1, 2], end=\"\\n\\n\")    # 第1行第2列, 即第1维的第1个元素的第2个元素\n",
    "print(\"X[1:3, 2:4] = \\n\", X[1:3, 2:4], end=\"\\n\\n\")  # 第1, 2行, 第2, 3列, 即第1维的第1个元素到第3个元素, 第2维的第2个元素到第4个元素\n",
    "\n",
    "# 可以发现:\n",
    "# 1. 用逗号隔开的索引, 代表对不同维度的索引\n",
    "# 2. 用冒号隔开的索引, 代表对同一维度的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在赋值时节省内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新赋值 (形如`X = X + Y`) 会为`X`重新分配一个地址. 我们希望避免这种写法\n",
    "原因:\n",
    "1. 避免不必要的内存分配, 尽量原地更新参数值\n",
    "2. 如果不原地更新, 其他引用仍然会指向旧的内存位置, 那么某些代码可能会无意中引用旧的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether the address of X changed if X = X + Y:  False\n",
      "Whether the address of X changed if X[:] = X + Y:  True\n",
      "Whether the address of X changed if X += Y:  True\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.arange(12, 24, dtype=torch.float32).reshape((3, 4))\n",
    "\n",
    "# X + Y 会重新分配内存\n",
    "before = id(X)\n",
    "X = X + Y\n",
    "after1 = id(X)\n",
    "print(\"Whether the address of X changed if X = X + Y: \", before == after1)\n",
    "\n",
    "# 使用切片表示法或+=号, 则不会重新分配\n",
    "before = id(X)\n",
    "X[:] = X + Y\n",
    "after2 = id(X)\n",
    "print(\"Whether the address of X changed if X[:] = X + Y: \", before == after2)\n",
    "\n",
    "before = id(X)\n",
    "X += Y\n",
    "after3 = id(X)\n",
    "print(\"Whether the address of X changed if X += Y: \", before == after3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
