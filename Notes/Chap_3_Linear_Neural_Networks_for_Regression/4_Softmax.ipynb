{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax回归\n",
    "\n",
    "回归：预测连续的数值\n",
    "\n",
    "分类：预测离散的数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 独热编码（One-Hot Encoding）\n",
    "\n",
    "分类问题中常用的编码方式，将一个数值转换为一个向量，向量的长度等于分类的个数，向量中只有一个元素为1，其他元素为0\n",
    "\n",
    "如：用 `[1, 0, 0], [0, 1, 0], [0, 0, 1]` 分别表示鸡、鸭、鹅三个类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络架构\n",
    "\n",
    "假设输入为2*2的灰度图像，输出为3个类别。那么输入特征数为4，输出特征数为3，此外还需要3个标量偏置项\n",
    "\n",
    "公式表示为:\n",
    "\n",
    "$$\n",
    "o_1 = x_1w_{11} + x_2w_{12} + x_3w_{13} + x_4w_{14} + b_1 \\\\\n",
    "o_2 = x_1w_{21} + x_2w_{22} + x_3w_{23} + x_4w_{24} + b_2 \\\\\n",
    "o_3 = x_1w_{31} + x_2w_{32} + x_3w_{33} + x_4w_{34} + b_3\n",
    "$$\n",
    "\n",
    "矢量化表示为:\n",
    "\n",
    "$$\n",
    "O = XW + b\n",
    "$$\n",
    "\n",
    "对于d维输入q维输出：\n",
    "\n",
    "$X$维度为`n*d`，$W$维度为`d*q`，$b$维度为`1*q`，$O$维度为`n*q`\n",
    "\n",
    "可以发现以上架构为一个全连接层，对于任何具有`d`个输入和`q`个输出的全连接层，参数开销为`O(dq)` ($W$的大小为`d*q`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax 运算\n",
    "\n",
    "上述线性层的输出存在两个问题：\n",
    "1. 总和不为1，无法表示概率\n",
    "2. 输出值可能为负数\n",
    "\n",
    "Softmax解决以上问题，将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(o) \\\\\n",
    "\\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_{i=1}^q \\exp(o_i)}\n",
    "$$\n",
    "\n",
    "尽管softmax是非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是一个**线性模型(linear model)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵损失\n",
    "\n",
    "目标与3.1节相似，最大化似然函数。等价于最小化交叉熵损失：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-log P(Y|X) \n",
    "&= -\\sum_{i=1}^n \\log P(y^{(i)}|x^{(i)})\\\\\n",
    "&= -\\sum_{i=1}^n l(y^{(i)}, \\hat{y}^{(i)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$l(y, \\hat{y})$是交叉熵损失函数(cross‐entropy loss)：\n",
    "\n",
    "$$\n",
    "l(y, \\hat{y}) = -\\sum_j^q y_j \\log \\hat{y}_j\n",
    "$$\n",
    "\n",
    "代入softmax函数并求导：\n",
    "\n",
    "$$\n",
    "l(y, \\hat{y}) = log \\sum_j^q \\exp(o_k) - \\sum_j^q y_j o_j\\\\\n",
    "\\partial_{o_j} l(y, \\hat{y}) = softmax(o)_j - y_j\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
