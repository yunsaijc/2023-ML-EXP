{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数值稳定性与模型初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度消失\n",
    "\n",
    "考虑一个具有 $L$ 层、输入 $x$ 和输出 $o$ 的深层网络。每一层 $l$ 由变换 $f_l$ 定义，该变换的参数为权重 $w^{(l)}$，其隐藏变量为 $h^{(l)}$（令$h^{(0)} = x$）。则网络可以表示为：\n",
    "$$\n",
    "h^{(l)} = f_l(h^{(l-1)}), \\quad o = f_L \\circ \\ldots \\circ f_1(x)\n",
    "$$\n",
    "\n",
    "那么，$o$ 关于 $w^{(l)}$ 的梯度为：\n",
    "$$\n",
    "\\partial_{w^{(l)}} o = \\underbrace{\\partial_{h^{(L)}} f_L \\ldots \\partial_{h^{(l+1)}} f_{l+1}}_{\\text{从 $l+1$ 到 $L$ 层的梯度传播}} \\cdot \\partial_{w^{(l)}} f_l(h^{(l-1)})\n",
    "$$\n",
    "\n",
    "也就是说，该梯度是 $L-l$ 个矩阵乘积的结果。\n",
    "\n",
    "因此如果是概率矩阵这类取值在 $[0, 1]$ 之间的矩阵，会受到数值下溢的影响，导致梯度消失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度爆炸\n",
    "\n",
    "与梯度消失相反，梯度爆炸会导致梯度变得非常大。\n",
    "\n",
    "如果这种情况是由于深度网络的初始化所导致的，那就没有机会让梯度下降优化器收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier初始化\n",
    "\n",
    "为了解决梯度消失和梯度爆炸问题，我们可以使用 Xavier 初始化方法。该方法假设每一层的输入和输出是高斯分布的，且方差相等。因此，我们可以通过以下方式初始化权重：\n",
    "$$\n",
    "w^{(l)} \\sim \\mathcal{N} \\left(0, \\frac{2}{n^{(l-1)} + n^{(l)}} \\right)\n",
    "$$\n",
    "其中 $n^{(l)}$ 是第 $l$ 层的输入个数。\n",
    "\n",
    "具体推导见参考材料"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
